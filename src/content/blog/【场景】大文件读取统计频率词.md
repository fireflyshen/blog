---
author: firefly
pubDatetime: 2024-12-03T15:22:00Z
modDatetime: 2024-12-03T15:22:00Z
title: 【场景】大文件读取统计频率词
slug: bigfile
featured:
draft:
tags:
  - 场景
  - 大文件读取
description:
aliases:
---

最近面试经常被问到的一个场景问题，大文件读取的问题，题目大致如下

> [!TIp]
> 给你一个2g内存大小的机器，然后目前有一个10g的文件，你要将他内部中出现频率最高的词汇统计出来

这种题目其实有一套非常套路的回答就是将文件分成一个一个的`chunk`然后分批次的加载进入内存之后进行读取。

当然打出这些肯定是不行的，他还要一些细节，比如，你怎么分批次读取，有的甚至还要什么优化，为了回答这些问题，所有有了这篇文章

## 数据分片

首先第一步要考虑的就是数据分片，因为内存是刚性的所以这里不能一下子将所有文件读取到内存中。这里要考虑分片

```java

private static List<File>  splitAndCount(String inputFile, String tempDir) throws IOException {
    List<File> files = new ArrayList<File>();
    BufferedReader reader = new BufferedReader(new FileReader(inputFile));
    String line;
    Map<String, Long> wordCount = new HashMap<>();
    long currentChunkSize = 0;

    while ((line = reader.readLine())!= null){
        String[] words = line.split("\\W+");

        for (String word : words) {
            if (word.isEmpty()) continue;
            String lowerCase = word.toLowerCase();
            wordCount.put(lowerCase,wordCount.getOrDefault(lowerCase,0L) + 1);
            currentChunkSize += word.length();

            if (currentChunkSize >= CHUNK_SIZE) {
                files.add(writeTempFile(wordCount, tempDir));
                currentChunkSize = 0;
                wordCount.clear();
            }

        }

        if (!wordCount.isEmpty()) {
            files.add(writeTempFile(wordCount, tempDir));
        }
        reader.close();
    }
  ···
    return files;
}
```

以上就是分片函数的整体

1. 一行一行的读取文件

```java
while ((line = reader.readLine())!= null){
```

2. 将读取的文件分割成数组，根据正则

```java
 String[] words = line.split("\\W+");
```

3. 循环数组，统计次数

```java
 if (word.isEmpty()) continue;
            String lowerCase = word.toLowerCase();
            wordCount.put(lowerCase,wordCount.getOrDefault(lowerCase,0L) + 1);
            currentChunkSize += word.length();

if (currentChunkSize >= CHUNK_SIZE) {
	files.add(writeTempFile(wordCount, tempDir));
	currentChunkSize = 0;
	wordCount.clear();
}

```

就是这里对数据完成的分片，根据`currentChunkSize`如果数组的字节大于已经设定好的值，那么就会将已经统计好的值放入一个持久化的文件中，之后清空，从头开始，然后将文件放入创建好的LIst中返回

## 合并

合并其实很简单，因为上一个数据分片的函数已经把所有的文件都封装成List对象返回了，所以这里只需要遍历这个集合，然后将文件一行一行读取，根据特定的分隔符分割放入Map中即可

```java
private static Map<String, Long> mergeCounts(List<File> tempFiles) throws IOException {
    Map<String, Long> globalFrequency = new HashMap<>();
    for (File tempFile : tempFiles) {
        BufferedReader reader = new BufferedReader(new FileReader(tempFile));
        String line;
        while ((line = reader.readLine()) != null) {
            String[] parts = line.split(",");
            String word = parts[0];
            long count = Long.parseLong(parts[1]);
            globalFrequency.put(word, globalFrequency.getOrDefault(word, 0L) + count);
        }
        reader.close();
        tempFile.delete(); // 删除临时文件
    }
    return globalFrequency;
}
```

循环文件列表

```java
for (File tempFile : tempFiles) {
	BufferedReader reader = new BufferedReader(new FileReader(tempFile));
	String line;
	while ((line = reader.readLine()) != null) {
		String[] parts = line.split(",");
		String word = parts[0];
		long count = Long.parseLong(parts[1]);
		globalFrequency.put(word, globalFrequency.getOrDefault(word, 0L) + count);

	}
	reader.close();
	tempFile.delete(); // 删除临时文件
}
```

之后就是一个循环Map找最大值的过程了

```java
private static String findMaxFrequencyWord(Map<String, Long> frequencyMap) {
    String mostFrequentWord = null;
    long maxFrequency = 0;
    for (Map.Entry<String, Long> entry : frequencyMap.entrySet()) {
        if (entry.getValue() > maxFrequency) {
            maxFrequency = entry.getValue();
            mostFrequentWord = entry.getKey();
        }
    }
    return mostFrequentWord + " (" + maxFrequency + " times)";
}
```

以上一个大文件的
